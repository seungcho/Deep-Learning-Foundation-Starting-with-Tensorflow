{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lab 02: Simple Linear Regression를 Tensorflow로 구현하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Build hypothesis and cost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/lb2-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**머신러닝 학습은 cost가 최소화가 되는 W(가중치)와 b(편향)을 찾는것이다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost\n",
    "# cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "# tf.reduce_mean(): rank(차원)가 줄어들면서 mean을 구한다.\n",
    "# v = [1.,2.,3.,4.]\n",
    "# tf.reduce_mean(v) # 2.5\n",
    "# tf.square()\n",
    "# tf.square(3) # 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **경사 하강법(Gradient Descent)소개**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**W 파라미터의 개수가 적다면 고차원 방정식으로 비용 함수가 최소가 되는 W 변숫값을 도출할 수 있겠지만, W 파라미터가 많으면 고차원 방정식을 동원하더라도 해결하기가 어렵다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**경사 하강법은 이러한 고차원 방정식에 대한 문제를 해결해 주면서 비용 함수 RSS를 최소화하는 방법을 직관적으로 제공하는 뛰어난 방식이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**사실 경사 하강법은 '데이터를 기반으로 알고리즘이 스스로 학습한다'는 머신러닝의 개념을 가능하게 만들어준 핵심 기법의 하나이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**경사 하강법의 사전적 의미인 '점진적인 하강'이라는 뜻에서도 알 수 있듯이, '점진적으로' 반복적인 계산을 통해 W 파라미터 값을 업데이트하면서 오류 값이 최소가 되는 W 파라미터를 구하는 방식이다.**\n",
    "\n",
    "> **어떻게 보면 무식해 보이는 방법이지만 W 파라미터의 개수에 따라 매우 복잡해지는 고차원 방정식을 푸는 것보다 훨씬 더 직관적이고 빠르게 비용 함수가 최소가 되는 W파라미터 값을 구할 수 있다.**\n",
    "\n",
    "**경사 하강법은 반복적으로 비용 함수의 반환 값, 즉 예측값과 실제 값의 차이가 작아지는 방향성을 가지고 W 파라미터를 지속해서 보정해 나간다.**\n",
    "\n",
    "> **최초 오류 값이 100이었다면 두 번째 오류 값은 100보다 작은 90, 세 번째는 80과 같은 방식으로 지속해서 오류를 감소시키는 방향으로 W 값을 계속 업데이트해 나간다.**\n",
    "\n",
    "> **그리고 오류 값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그때의 W값을 최적 파라미터로 반환한다.**\n",
    "\n",
    "**경사 하강법의 핵심은 \"어떻게 하면 오류가 작아지는 방향으로 W 값을 보정할 수 있을까?\"이다.**\n",
    "\n",
    "> **예를 들어 비용 함수가 포물선 형태의 2차 함수라면 경사 하강법은 최초 W 에서 부터 미분을 적용한 뒤 이 미분 값이 계속 감소하는 방향으로 순차적으로 W를 업데이트한다.**\n",
    "\n",
    "> **더 이상 미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용 함수가 최소인 지점으로 간주하고 그때의 W를 반환한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/경사하강법.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/경사하강법1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **step1: w1를 임의의 값으로 설정(bias 생략)하고 첫 비용함수의 값을 계산한다.**\n",
    "\n",
    "> **step2: w1 을 위 그림과 같은 식으로 업데이트한 후 다시 비용 함수의 값을 계산한다.**\n",
    "\n",
    "> **step3: 비용 함수의 값이 감소했으면 다시 step 2를 반복한다. 더 이상 비용 함수의 값이 감소하지 않으면 그때의 w1를 구하고 반복을 중지한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|    2.4520|    0.3760| 45.660004\n",
      "   10|    1.1036|    0.0034|  0.206336\n",
      "   20|    1.0128|   -0.0209|  0.001026\n",
      "   30|    1.0065|   -0.0218|  0.000093\n",
      "   40|    1.0059|   -0.0212|  0.000083\n",
      "   50|    1.0057|   -0.0205|  0.000077\n",
      "   60|    1.0055|   -0.0198|  0.000072\n",
      "   70|    1.0053|   -0.0192|  0.000067\n",
      "   80|    1.0051|   -0.0185|  0.000063\n",
      "   90|    1.0050|   -0.0179|  0.000059\n",
      "tf.Tensor(5.0066934, shape=(), dtype=float32)\n",
      "tf.Tensor(2.4946523, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Data set\n",
    "x_data = [1,2,3,4,5]\n",
    "y_data = [1,2,3,4,5]\n",
    "\n",
    "# init weight, bias\n",
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "# H(x) = Wx + b\n",
    "hypothesis = W*x_data + b\n",
    "\n",
    "# Gradient descent\n",
    "# learning_rate initialize\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Gradient descent를 총 100번을 반복하기 위한 코드\n",
    "for i in range(100): \n",
    "    # Gradient descent(W,b가 한번 업데이트를 위한 코드)\n",
    "    # with구문 안에 있는 변수들의 정보를 tape에 저장하게 된다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W * x_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "    # cost값에 대한 개별 파라미터(w,b)의 미분 값을 w_grad, b_grad에 반환\n",
    "    W_grad, b_grad = tape.gradient(cost, [W,b])\n",
    "\n",
    "    # Weight update\n",
    "    # W(업데이트된 Weight) -= learning_rate * W(현재의 W의 기울기)\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    # 10의 배수가 될때 마다 중간 값들을 확인\n",
    "    if i % 10 == 0:\n",
    "        print('{:5}|{:10.4f}|{:10.4f}|{:10.6f}'.format(i,W.numpy(),b.numpy(),cost))\n",
    "\n",
    "# Predict\n",
    "print(W * 5 + b)\n",
    "print(W * 2.5 + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/lb2-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**초기값 W = 2.9, b = 0.5에서 반복수를 1로 하여 한번 학습된 회귀직선을 보면 기존 데이터에 제대로 학습되지 않은 결과를 볼 수 있고, 경사하강법을 통해 반복수를 90번으로 했을 때 오른쪽 그림처럼 기존 데이터를 잘 학습하여 최적의 회귀직선이 구해짐을 알 수 있다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
